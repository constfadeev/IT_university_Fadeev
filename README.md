# IT_university_task

Добрый день. 
Здесь представлено мое решение входного задания на курс по "Анализу данных средствами Python" пермского сетевого IT университета.

**Задание:**
Разработать программу на языке программирования Python, которая выделяет ключевые слова в заданном русскоязычном тексте. Использовать частотный анализ слов в тексте. За приведение слов к нормальной форме и исключение стоп-слов можно получить дополнительные баллы. Разрешается использования сторонних библиотек.
*Входные данные* – текстовый файл с расширением txt с заданным текстом, который вы можете выбрать самостоятельно (путь к файлу задается в консоли).
*Выходные данные* – таблица Excel с упорядоченным списком ключевых слов.

## Решение
Для выделения ключевых слов использовалась библиотека [**rutermextract**](https://pypi.org/project/rutermextract/), находящаяся в свободном доступе. Также использовалась библиотека **pandas** для удобной работы с выходными данными. 

Сначала импортируются библиотеки для работы и создается пустой датафрейм, куда в дальнейшем будет записываться результат.
```python
import pandas as pd
from rutermextract import TermExtractor
import codecs

term_extractor = TermExtractor()
df = pd.DataFrame(columns=['keyword', 'frequency'])
```
В консоль выводится сообщение с предложением пользователю написать путь до файла. При указании правильного пути, файл считывается в переменную *text*, при этом все переносы строк удаляются.
```python
path = input('Введите путь к файлу: ')
with codecs.open(path, 'r', 'utf_8_sig') as file:
    text = file.read().replace('\n', '')
```
Текст обрабатывается библиотекой *rutermextract*, которая проводит стандартные операции для поиска ключевых слов (токенизация, удаление знаков пунктуации, нормализация, удаление стоп-слов). В цикле в датафрейм записываются ключевые слова в нормализованной форме в колонку *keywords*, а в колонку *frequency* записывается количество их употреблений в тексте. Аргумент *nested=True* позволяет извлекать ключевые слова, лежащие внтури лругих ключевых слов. 
```python
i = 0
for term in term_extractor(text, nested=True):
    df.loc[i] = [term.normalized, term.count]
    i += 1
```
Даже если слово употребляется только один раз, оно будет занесено в качестве ключевого слова в датафрейм. При большом объеме документа таких слов может быть очень много. Чтобы не показывать пользователю список из огромного количества слов, можно его ограничить и выводить только самые встречаемые. Переменная *top_percent* определяет какую часть слов выводить пользователю. При ее значении 0.1 пользователю будет выведены самые популярные ключевые слова в размере 10% от всего количества слов. 
Далее происходит сохранение датафрейма в формате Excel в ту папку, где находится файл, указанный пользователем. В файле в первом столбце упорядоченный по частоте список ключевых слов, а во втором столбце их частота употребления в тексте.
```python
top_percent = 0.1

df_top = df.loc[df.frequency > df.loc[int(df.shape[0]*top_percent)].frequency]
df_top.to_excel(f'{path[:path.rfind(".")] + "_"}keywords.xlsx', index=False)
```

## Пример
Для примера, возьмем [статью](https://www.permkrai.ru/news/permskiy-setevoy-it-universitet-nabiraet-slushateley/) о пермском IT-университете на сайте губернатора и правительства Пермского края. Прогоним ее через написанный скрипт.
На выходе получим следующие результаты:
|keyword | frequency|
|--------|----------|
|it-университет|7|
|специалисты|5|
|проект|4|
|пермский сетевой it-университет|3|
|сетевой it-университет|3|
|университет|3|
|программа|3|
|образование|3|
|it-компании|3|
|galileosky|3|
